{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed, retries\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "print(\"ðŸš€ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True, dotenv_path=\".env\")\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "    print(\"Environment variables are set.\")\n",
    "\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! ðŸš€\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! âš¡ï¸\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. ðŸŽ‰\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. ðŸŽ‰\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "easyllm = EasyLLM()\n",
    "\n",
    "DEFAULT_PRD_TOKENS = 8192\n",
    "\n",
    "def generate_initial_prd(prompt: str) -> str:\n",
    "    initial_prompt = f\"Generate a Product Requirement Document for: '{prompt}'\"\n",
    "    system = \"You are a Product Manager. The engineering team is waiting for a PRD for a new feature. Respond in markdown.\"\n",
    "    return easyllm.generate_text(initial_prompt, system, max_tokens=DEFAULT_PRD_TOKENS)\n",
    "\n",
    "\n",
    "def generate_prd_questions(prompt: str, prd: str) -> str:\n",
    "    questions_prompt = f\"Given a PRD for '{prompt}', generate a set of questions that can be asked to further refine the PRD.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(questions_prompt, max_tokens=DEFAULT_PRD_TOKENS // 2)\n",
    "\n",
    "\n",
    "def generate_prd_answers(prompt: str, prd: str, questions: str) -> str:\n",
    "    answers_prompt = f\"Given a PRD for '{prompt}', and some questions: '{questions}', generate a set of answers to the questions.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(answers_prompt, max_tokens=DEFAULT_PRD_TOKENS // 2)\n",
    "\n",
    "\n",
    "def generate_final_prd(prompt: str, prd: str, questions: str, answers: str) -> str:\n",
    "    final_prd_prompt = f\"Given a PRD for '{prompt}', and some questions: '{questions}', and answers: '{answers}', generate a final PRD.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(final_prd_prompt, max_tokens=DEFAULT_PRD_TOKENS)\n",
    "\n",
    "\n",
    "def prd_generation(prompt: str) -> str:\n",
    "    \"\"\"Generates a PRD using the EasyLLM abstraction.\"\"\"\n",
    "    try:\n",
    "        print(f\"Generating PRD for: '{prompt}'\")\n",
    "        prd = generate_initial_prd(prompt)\n",
    "        if not prd:\n",
    "            raise Exception(\"Failed to generate PRD.\")\n",
    "\n",
    "        print(f\"Generated PRD: {prd}\")\n",
    "        questions = generate_prd_questions(prompt, prd)\n",
    "        if not questions:\n",
    "            raise Exception(\"Failed to generate PRD questions.\")\n",
    "\n",
    "        print(f\"Generated PRD questions: {questions}\")\n",
    "        answers = generate_prd_answers(prompt, prd, questions)\n",
    "        if not answers:\n",
    "            raise Exception(\"Failed to generate PRD answers.\")\n",
    "\n",
    "        print(f\"Generated PRD answers: {answers}\")\n",
    "        final_prd = generate_final_prd(prompt, prd, questions, answers)\n",
    "        if not final_prd:\n",
    "            raise Exception(\"Failed to generate final PRD.\")\n",
    "\n",
    "        print(f\"Generated final PRD: {final_prd}\")\n",
    "        return final_prd\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate PRD. Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Failed to generate PRD.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "prompt = \"Instagram-like music sharing app for artists.\"\n",
    "\n",
    "prd = prd_generation(prompt)\n",
    "\n",
    "clear_output()\n",
    "display(Markdown(prd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
