{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered PRD Generator\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://cdn.prod.website-files.com/63fc977c14aaea404dce4439/66bdc2679e2664f695587a96_64889c8bce5099abb0406468_image%25201.webp\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "#### An AI-driven tool that automates the creation and refinement of Product Requirement Documents (PRDs), ensuring clear and structured documentation for engineering teams.\n",
    "\n",
    "##### Key Features:\n",
    "\n",
    "\n",
    "- **Automated PRD Creation** – Generates an initial PRD based on a given prompt.\n",
    "\n",
    "- **Refinement through Questions** – Identifies gaps by generating relevant questions.\n",
    "\n",
    "- **Smart Answer Generation** – Provides AI-driven responses to refine PRD details.\n",
    "\n",
    "- **Final PRD Generation** – Combines insights into a polished, structured PRD.\n",
    "\n",
    "- **Markdown Output** – Ensures readability and easy collaboration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Installation\n",
    "\n",
    "This step installs dependencies from `requirements.txt` and checks for `OPENAI_API_KEY`.  \n",
    "\n",
    "If installation fails, it retries up to 3 times before exiting.  \n",
    "\n",
    "Once complete, it clears the output and prints a success message.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed, retries\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "print(\"🚀 Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Variable Setup\n",
    "\n",
    "This step loads environment variables from `.env` using `dotenv`.  \n",
    "\n",
    "It checks if `OPENAI_API_KEY` is set; if missing, it exits.  \n",
    "\n",
    "After validation, it confirms successful setup.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True, dotenv_path=\".env\")\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "    print(\"Environment variables are set.\")\n",
    "\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: OpenAI API Abstraction\n",
    "\n",
    "This class **EasyLLM** simplifies interaction with the OpenAI API.  \n",
    "\n",
    "It initializes with API credentials, model settings, and verbosity/debug options.  \n",
    "\n",
    "- `generate_text` generates text based on a given prompt and system message.  \n",
    "\n",
    "- `generate_object` returns structured responses using Pydantic models.  \n",
    "\n",
    "Methods to **get** and **set** the model are also included.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! 🚀\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ⚡️\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. 🎉\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. 🎉\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: PRD Generation\n",
    "\n",
    "This module automates Product Requirement Document (PRD) creation.  \n",
    "\n",
    "- `generate_initial_prd` drafts the first PRD using a prompt.  \n",
    "\n",
    "- `generate_prd_questions` generates clarifying questions for refinement.  \n",
    "\n",
    "- `generate_prd_answers` provides answers to those questions.  \n",
    "\n",
    "- `generate_final_prd` compiles everything into a final PRD.  \n",
    "\n",
    "- `prd_generation` orchestrates the entire process and handles errors.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "easyllm = EasyLLM()\n",
    "\n",
    "DEFAULT_PRD_TOKENS = 8192\n",
    "\n",
    "def generate_initial_prd(prompt: str) -> str:\n",
    "    initial_prompt = f\"Generate a Product Requirement Document for: '{prompt}'\"\n",
    "    system = \"You are a Product Manager. The engineering team is waiting for a PRD for a new feature. Respond in markdown.\"\n",
    "    return easyllm.generate_text(initial_prompt, system, max_tokens=DEFAULT_PRD_TOKENS)\n",
    "\n",
    "\n",
    "def generate_prd_questions(prompt: str, prd: str) -> str:\n",
    "    questions_prompt = f\"Given a PRD for '{prompt}', generate a set of questions that can be asked to further refine the PRD.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(questions_prompt, max_tokens=DEFAULT_PRD_TOKENS // 2)\n",
    "\n",
    "\n",
    "def generate_prd_answers(prompt: str, prd: str, questions: str) -> str:\n",
    "    answers_prompt = f\"Given a PRD for '{prompt}', and some questions: '{questions}', generate a set of answers to the questions.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(answers_prompt, max_tokens=DEFAULT_PRD_TOKENS // 2)\n",
    "\n",
    "\n",
    "def generate_final_prd(prompt: str, prd: str, questions: str, answers: str) -> str:\n",
    "    final_prd_prompt = f\"Given a PRD for '{prompt}', and some questions: '{questions}', and answers: '{answers}', generate a final PRD.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(final_prd_prompt, max_tokens=DEFAULT_PRD_TOKENS)\n",
    "\n",
    "\n",
    "def prd_generation(prompt: str) -> str:\n",
    "    \"\"\"Generates a PRD using the EasyLLM abstraction.\"\"\"\n",
    "    try:\n",
    "        print(f\"Generating PRD for: '{prompt}'\")\n",
    "        prd = generate_initial_prd(prompt)\n",
    "        if not prd:\n",
    "            raise Exception(\"Failed to generate PRD.\")\n",
    "\n",
    "        print(f\"Generated PRD: {prd}\")\n",
    "        questions = generate_prd_questions(prompt, prd)\n",
    "        if not questions:\n",
    "            raise Exception(\"Failed to generate PRD questions.\")\n",
    "\n",
    "        print(f\"Generated PRD questions: {questions}\")\n",
    "        answers = generate_prd_answers(prompt, prd, questions)\n",
    "        if not answers:\n",
    "            raise Exception(\"Failed to generate PRD answers.\")\n",
    "\n",
    "        print(f\"Generated PRD answers: {answers}\")\n",
    "        final_prd = generate_final_prd(prompt, prd, questions, answers)\n",
    "        if not final_prd:\n",
    "            raise Exception(\"Failed to generate final PRD.\")\n",
    "\n",
    "        print(f\"Generated final PRD: {final_prd}\")\n",
    "        return final_prd\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate PRD. Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Failed to generate PRD.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: PRD Display  \n",
    "\n",
    "This step generates and displays a PRD in markdown format by defining the product concept, generating the PRD, clearing unnecessary logs, and presenting the final output in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "prompt = \"Instagram-like music sharing app for artists.\"\n",
    "\n",
    "prd = prd_generation(prompt)\n",
    "\n",
    "clear_output()\n",
    "display(Markdown(prd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "\n",
    "This app streamlines PRD generation using AI, making product documentation faster and more structured.\n",
    "\n",
    "It automates key steps — initial PRD creation, refinement through questions, and finalization — ensuring clarity for engineering teams. \n",
    "\n",
    "The markdown output enhances readability, making collaboration easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! 🌐\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
