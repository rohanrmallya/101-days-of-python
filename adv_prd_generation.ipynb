{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered PRD Generator\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://cdn.prod.website-files.com/63fc977c14aaea404dce4439/66bdc2679e2664f695587a96_64889c8bce5099abb0406468_image%25201.webp\"> \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "## Description:\n",
    "\n",
    "#### An AI-driven tool that automates the creation and refinement of Product Requirement Documents (PRDs), ensuring clear and structured documentation for engineering teams.\n",
    "\n",
    "##### Key Features:\n",
    "\n",
    "\n",
    "- **Automated PRD Creation** ‚Äì Generates an initial PRD based on a given prompt.\n",
    "\n",
    "- **Refinement through Questions** ‚Äì Identifies gaps by generating relevant questions.\n",
    "\n",
    "- **Smart Answer Generation** ‚Äì Provides AI-driven responses to refine PRD details.\n",
    "\n",
    "- **Final PRD Generation** ‚Äì Combines insights into a polished, structured PRD.\n",
    "\n",
    "- **Markdown Output** ‚Äì Ensures readability and easy collaboration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Installation\n",
    "\n",
    "This step installs dependencies from `requirements.txt` and checks for `OPENAI_API_KEY`.  \n",
    "\n",
    "If installation fails, it retries up to 3 times before exiting.  \n",
    "\n",
    "Once complete, it clears the output and prints a success message.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate: This block goes into every notebook.\n",
    "# It sets up the environment, installs the requirements, and checks for the required environment variables.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "requirements_installed = False\n",
    "max_retries = 3\n",
    "retries = 0\n",
    "REQUIRED_ENV_VARS = [\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"Installs the requirements from requirements.txt file\"\"\"\n",
    "    global requirements_installed, retries\n",
    "    if requirements_installed:\n",
    "        print(\"Requirements already installed.\")\n",
    "        return\n",
    "\n",
    "    print(\"Installing requirements...\")\n",
    "    install_status = os.system(\"pip install -r requirements.txt\")\n",
    "    if install_status == 0:\n",
    "        print(\"Requirements installed successfully.\")\n",
    "        requirements_installed = True\n",
    "    else:\n",
    "        print(\"Failed to install requirements.\")\n",
    "        if retries < max_retries:\n",
    "            print(\"Retrying...\")\n",
    "            retries += 1\n",
    "            return install_requirements()\n",
    "        exit(1)\n",
    "    return\n",
    "\n",
    "\n",
    "install_requirements()\n",
    "clear_output()\n",
    "print(\"üöÄ Setup complete. Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Environment Variable Setup\n",
    "\n",
    "This step loads environment variables from `.env` using `dotenv`.  \n",
    "\n",
    "It checks if `OPENAI_API_KEY` is set; if missing, it exits.  \n",
    "\n",
    "After validation, it confirms successful setup.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def setup_env():\n",
    "    \"\"\"Sets up the environment variables\"\"\"\n",
    "\n",
    "    def check_env(env_var):\n",
    "        value = os.getenv(env_var)\n",
    "        if value is None:\n",
    "            print(f\"Please set the {env_var} environment variable.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            print(f\"{env_var} is set.\")\n",
    "\n",
    "    load_dotenv(override=True, dotenv_path=\".env\")\n",
    "\n",
    "    variables_to_check = REQUIRED_ENV_VARS\n",
    "\n",
    "    for var in variables_to_check:\n",
    "        check_env(var)\n",
    "\n",
    "    print(\"Environment variables are set.\")\n",
    "\n",
    "\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: OpenAI API Abstraction\n",
    "\n",
    "This class **EasyLLM** simplifies interaction with the OpenAI API.  \n",
    "\n",
    "It initializes with API credentials, model settings, and verbosity/debug options.  \n",
    "\n",
    "- `generate_text` generates text based on a given prompt and system message.  \n",
    "\n",
    "- `generate_object` returns structured responses using Pydantic models.  \n",
    "\n",
    "Methods to **get** and **set** the model are also included.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "import traceback\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are an intelligent AI assistant. The user will give you a prompt, respond appropriately.\"\n",
    "DEFAULT_TEMPERATURE = 0.5\n",
    "DEFAULT_MAX_TOKENS = 1024\n",
    "\n",
    "\n",
    "class EasyLLM:\n",
    "    \"\"\"\n",
    "    A simple abstraction for the OpenAI API. It provides easy-to-use methods to generate text and objects using the OpenAI API.\n",
    "    A demonstration for the \"How to build an Abstaction with Open AI API\" blog post.\n",
    "    Author: Aditya Patange (AdiPat)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=DEFAULT_OPENAI_MODEL,\n",
    "        verbose=True,\n",
    "        debug=True,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"EasyLLM: Powering up! üöÄ\")\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.openai = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"EasyLLM: Model set to {model}.\")\n",
    "            print(\"EasyLLM: Ready for some Generative AI action! ‚ö°Ô∏è\")\n",
    "\n",
    "    def generate_text(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[str, None]:\n",
    "        \"\"\"Generates text using the OpenAI API.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating text for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "            response = self.openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            response = response.choices[0].message.content\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Text generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response = json.dumps(response)\n",
    "                print(f\"EasyLLM Response: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate text. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def generate_object(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_model: BaseModel,\n",
    "        system=DEFAULT_SYSTEM_PROMPT,\n",
    "        temperature=DEFAULT_TEMPERATURE,\n",
    "        max_tokens=DEFAULT_MAX_TOKENS,\n",
    "    ) -> Union[BaseModel, None]:\n",
    "        \"\"\"Generates an object using the OpenAI API and given response model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Generating object for prompt: {prompt}\")\n",
    "\n",
    "            if self.debug:\n",
    "                params = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"system\": system,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"model\": self.model,\n",
    "                }\n",
    "                params = json.dumps(params, indent=2)\n",
    "                print(f\"Params: {params}\")\n",
    "\n",
    "            response = self.openai.beta.chat.completions.parse(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_model,\n",
    "                model=self.model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "\n",
    "            if self.verbose or self.debug:\n",
    "                print(\"Object generated successfully. üéâ\")\n",
    "\n",
    "            if self.debug:\n",
    "                response_json = response.model_dump_json()\n",
    "                print(f\"EasyLLM Response: {response_json}\")\n",
    "            return response.choices[0].message.parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate object. Error: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def get_model(self) -> str:\n",
    "        \"\"\"Gets the current model.\"\"\"\n",
    "        return self.model\n",
    "\n",
    "    def set_model(self, model: str) -> None:\n",
    "        \"\"\"Sets the model to the given model.\"\"\"\n",
    "        try:\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Setting model to {model}\")\n",
    "            self.openai = openai.OpenAI(api_key=self.api_key)\n",
    "            self.model = model\n",
    "            if self.verbose or self.debug:\n",
    "                print(f\"Model set to {model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to set model.\\nError: {str(e)}\")\n",
    "            if self.debug:\n",
    "                traceback.print_exc()\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: PRD Generation\n",
    "\n",
    "This module automates Product Requirement Document (PRD) creation.  \n",
    "\n",
    "- `generate_initial_prd` drafts the first PRD using a prompt.  \n",
    "\n",
    "- `generate_prd_questions` generates clarifying questions for refinement.  \n",
    "\n",
    "- `generate_prd_answers` provides answers to those questions.  \n",
    "\n",
    "- `generate_final_prd` compiles everything into a final PRD.  \n",
    "\n",
    "- `prd_generation` orchestrates the entire process and handles errors.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "easyllm = EasyLLM()\n",
    "\n",
    "DEFAULT_PRD_TOKENS = 8192\n",
    "\n",
    "def generate_initial_prd(prompt: str) -> str:\n",
    "    initial_prompt = f\"Generate a Product Requirement Document for: '{prompt}'\"\n",
    "    system = \"You are a Product Manager. The engineering team is waiting for a PRD for a new feature. Respond in markdown.\"\n",
    "    return easyllm.generate_text(initial_prompt, system, max_tokens=DEFAULT_PRD_TOKENS)\n",
    "\n",
    "\n",
    "def generate_prd_questions(prompt: str, prd: str) -> str:\n",
    "    questions_prompt = f\"Given a PRD for '{prompt}', generate a set of questions that can be asked to further refine the PRD.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(questions_prompt, max_tokens=DEFAULT_PRD_TOKENS // 2)\n",
    "\n",
    "\n",
    "def generate_prd_answers(prompt: str, prd: str, questions: str) -> str:\n",
    "    answers_prompt = f\"Given a PRD for '{prompt}', and some questions: '{questions}', generate a set of answers to the questions.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(answers_prompt, max_tokens=DEFAULT_PRD_TOKENS // 2)\n",
    "\n",
    "\n",
    "def generate_final_prd(prompt: str, prd: str, questions: str, answers: str) -> str:\n",
    "    final_prd_prompt = f\"Given a PRD for '{prompt}', and some questions: '{questions}', and answers: '{answers}', generate a final PRD.\\nPRD: '{prd}'\"\n",
    "    return easyllm.generate_text(final_prd_prompt, max_tokens=DEFAULT_PRD_TOKENS)\n",
    "\n",
    "\n",
    "def prd_generation(prompt: str) -> str:\n",
    "    \"\"\"Generates a PRD using the EasyLLM abstraction.\"\"\"\n",
    "    try:\n",
    "        print(f\"Generating PRD for: '{prompt}'\")\n",
    "        prd = generate_initial_prd(prompt)\n",
    "        if not prd:\n",
    "            raise Exception(\"Failed to generate PRD.\")\n",
    "\n",
    "        print(f\"Generated PRD: {prd}\")\n",
    "        questions = generate_prd_questions(prompt, prd)\n",
    "        if not questions:\n",
    "            raise Exception(\"Failed to generate PRD questions.\")\n",
    "\n",
    "        print(f\"Generated PRD questions: {questions}\")\n",
    "        answers = generate_prd_answers(prompt, prd, questions)\n",
    "        if not answers:\n",
    "            raise Exception(\"Failed to generate PRD answers.\")\n",
    "\n",
    "        print(f\"Generated PRD answers: {answers}\")\n",
    "        final_prd = generate_final_prd(prompt, prd, questions, answers)\n",
    "        if not final_prd:\n",
    "            raise Exception(\"Failed to generate final PRD.\")\n",
    "\n",
    "        print(f\"Generated final PRD: {final_prd}\")\n",
    "        return final_prd\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate PRD. Error: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return \"Failed to generate PRD.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: PRD Display  \n",
    "\n",
    "This step generates and displays a PRD in markdown format by defining the product concept, generating the PRD, clearing unnecessary logs, and presenting the final output in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "prompt = \"Instagram-like music sharing app for artists.\"\n",
    "\n",
    "prd = prd_generation(prompt)\n",
    "\n",
    "clear_output()\n",
    "display(Markdown(prd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "\n",
    "This app streamlines PRD generation using AI, making product documentation faster and more structured.\n",
    "\n",
    "It automates key steps ‚Äî initial PRD creation, refinement through questions, and finalization ‚Äî ensuring clarity for engineering teams. \n",
    "\n",
    "The markdown output enhances readability, making collaboration easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Thank You for visiting The Hackers Playbook! üåê\n",
    "\n",
    "If you liked this research material;\n",
    "\n",
    "- [Subscribe to our newsletter.](https://thehackersplaybook.substack.com)\n",
    "\n",
    "- [Follow us on LinkedIn.](https://www.linkedin.com/company/the-hackers-playbook/)\n",
    "\n",
    "- [Leave a star on our GitHub.](https://www.github.com/thehackersplaybook)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; padding: 50px;\">\n",
    "<p style=\"margin-right:10px;\">\n",
    "    <img height=\"200px\" style=\"width:auto;\" width=\"200px\" src=\"https://avatars.githubusercontent.com/u/192148546?s=400&u=95d76fbb02e6c09671d87c9155f17ca1e4ef8f21&v=4\"> \n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
